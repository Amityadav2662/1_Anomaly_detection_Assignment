{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d3ba0-7edb-4d7f-a182-d6ee96f2c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is anomaly detection and what is its purpose?\n",
    "Ans.\n",
    "Anomaly detection is a technique used in data analysis to identify outliers or unusual patterns in a dataset. Its purpose is to flag \n",
    "instances that deviate significantly from the norm or expected behavior. Anomalies could represent errors, outliers, or potentially\n",
    "interesting patterns that warrant further investigation.\n",
    "Anomaly detection is commonly used in various domains such as cybersecurity (to detect malicious activities), finance (to identify\n",
    "fraudulent transactions), manufacturing (to spot defects in products), and healthcare (to detect unusual patient conditions). By \n",
    "identifying anomalies, organizations can take proactive measures to address issues or exploit opportunities that may otherwise go unnoticed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c722e-4830-4f93-a2a7-3295ff6a063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What are the key challenges in anomaly detection?\n",
    "Ans.\n",
    "Anomaly detection faces several key challenges, including:\n",
    "1. Data Quality: Anomalies may be obscured by noisy or incomplete data. Poor data quality can make it difficult to distinguish true\n",
    "anomalies from random fluctuations or errors.\n",
    "2. Imbalanced Data: In many real-world scenarios, anomalies are rare compared to normal instances, resulting in imbalanced datasets. \n",
    "Traditional machine learning algorithms may struggle to effectively detect anomalies in such imbalanced data.\n",
    "3. High Dimensionality: Anomalies may not always be apparent in low-dimensional representations of the data. As the dimensionality of\n",
    "the data increases, the difficulty of detecting anomalies also increases due to the curse of dimensionality.\n",
    "4. Concept Drift: The underlying patterns in the data may change over time, leading to concept drift. Anomaly detection models trained\n",
    "on historical data may become less effective as new patterns emerge, requiring continuous monitoring and adaptation.\n",
    "5. Scalability: Anomaly detection algorithms must be scalable to handle large volumes of data efficiently. Real-time or near-real-time\n",
    "processing may be necessary in certain applications, posing additional scalability challenges.\n",
    "6. Adversarial Attacks: In cybersecurity and fraud detection, adversaries may deliberately manipulate data to evade detection. Anomaly \n",
    "detection models must be robust against such adversarial attacks to maintain their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a693d8-d57b-4941-a6c0-d2f8a9e08468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "Ans.\n",
    "Unsupervised anomaly detection and supervised anomaly detection are two different approaches to identifying anomalies in data:\n",
    "\n",
    "1. Unsupervised Anomaly Detection:\n",
    "In unsupervised anomaly detection, the algorithm is trained on a dataset containing only normal instances, without any labeled anomalies.\n",
    "The algorithm learns the underlying patterns or structure of the normal data and flags instances that deviate significantly from this\n",
    "learned pattern as anomalies.\n",
    "Unsupervised techniques include methods like clustering-based approaches, density estimation, and isolation forests.\n",
    "Unsupervised anomaly detection is useful when labeled anomaly data is scarce or unavailable, and it can identify previously unknown types\n",
    "of anomalies.\n",
    "\n",
    "2. Supervised Anomaly Detection:\n",
    "In supervised anomaly detection, the algorithm is trained on a dataset that includes both normal instances and labeled anomalies.\n",
    "The algorithm learns to distinguish between normal and anomalous instances based on the labeled examples provided during training.\n",
    "Supervised techniques typically involve training classifiers such as support vector machines (SVMs), decision trees, or neural networks.\n",
    "Supervised anomaly detection is effective when labeled anomaly data is available and can provide better performance compared to unsupervised \n",
    "methods, especially when anomalies are well-defined and representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055d273-9502-4d32-a21e-cb2811175cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are the main categories of anomaly detection algorithms?\n",
    "Ans.\n",
    "The main categories of anomaly detection algorithms are:\n",
    "1. Statistical Methods: These algorithms model the statistical properties of normal data and flag instances that deviate significantly from\n",
    "these properties.\n",
    "2. Machine Learning-Based Methods: These algorithms use machine learning techniques to learn patterns from normal data and identify anomalies\n",
    "based on deviations from these learned patterns.\n",
    "3. Proximity-Based Methods: These algorithms measure the similarity or dissimilarity between data points and identify anomalies as instances \n",
    "that are significantly different from their neighbors.\n",
    "4. Clustering-Based Methods: These algorithms group similar data points into clusters and identify anomalies as data points that do not belong\n",
    "to any cluster or belong to small clusters.\n",
    "5. Classification-Based Methods: These algorithms train classifiers to distinguish between normal and anomalous instances based on labeled \n",
    "training data, then classify new instances accordingly.\n",
    "6. Dimensionality Reduction-Based Methods: These algorithms reduce the dimensionality of the data while preserving its essential characteristics,\n",
    "making it easier to identify anomalies in the reduced-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6a2a8-bba9-4206-aa4a-99c0b066017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
    "Ans.\n",
    "Distance-based anomaly detection methods make the following main assumptions:\n",
    "1. Density: Normal instances are typically clustered together and have a higher density compared to anomalies.\n",
    "2. Distance: Anomalies are significantly distant from normal instances in the feature space, making them outliers with respect to the majority of \n",
    "the data points.\n",
    "3. Neighborhood: Anomalies have fewer or no neighboring data points in their vicinity, as they are isolated or distant from the majority of the\n",
    "data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb0d1ac-3623-4141-85a7-bce049cfeb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. How does the LOF algorithm compute anomaly scores?\n",
    "Ans.\n",
    "The Local Outlier Factor (LOF) algorithm computes anomaly scores for each data point based on its local density compared to the densities of its\n",
    "neighbors. Here's how it works:\n",
    "1. Local Density Estimation: For each data point, the algorithm computes its local density by measuring the average distance to its k nearest\n",
    "neighbors. A lower average distance indicates higher density, while a higher average distance suggests lower density.\n",
    "2. Reachability Distance: The reachability distance of a data point p with respect to its neighbor q is defined as the maximum of the distance between \n",
    "p and q and the local density of q. This distance measures how easily p can be reached from q while considering the local density around q.\n",
    "3. Local Reachability Density: The local reachability density of a data point is defined as the inverse of the average reachability distance to its\n",
    "k nearest neighbors. Higher values indicate that the data point is more reachable from its neighbors and thus has lower outlierliness.\n",
    "4. Local Outlier Factor (LOF) Score: Finally, the LOF score of a data point is computed by comparing its local reachability density with that of its\n",
    "neighbors. A high LOF score indicates that the data point is significantly less dense than its neighbors, suggesting that it may be an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79338e5-4459-4d08-af39-eb4fdd6ab32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. What are the key parameters of the Isolation Forest algorithm?\n",
    "Ans.\n",
    "The Isolation Forest algorithm, a popular tree-based anomaly detection method, has a few key parameters that can be adjusted to control its behavior:\n",
    "1. Number of Trees (n_estimators): This parameter specifies the number of isolation trees to be created in the forest. A larger number of trees can \n",
    "lead to better detection of anomalies but may also increase computational complexity.\n",
    "2. Maximum Tree Depth (max_depth): The maximum depth allowed for each isolation tree in the forest. Controlling the maximum depth can help prevent \n",
    "overfitting and improve the algorithm's generalization ability.\n",
    "3. Subsample Size (max_samples): This parameter determines the number of samples randomly drawn from the dataset to build each isolation tree. A \n",
    "smaller subsample size can lead to faster training but may also reduce the algorithm's effectiveness in capturing the underlying data structure.\n",
    "4. Contamination: This parameter specifies the expected proportion of anomalies in the dataset. It is used to adjust the decision threshold for \n",
    "classifying instances as anomalies. A higher contamination value indicates a higher threshold for identifying anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a64ee75-58d1-40e8-ba26-4476ae121799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "# using KNN with K=10?\n",
    "To compute the anomaly score of a data point using K-nearest neighbors (KNN) with K=10, we need to identify the distance between the data point and \n",
    "its 10th nearest neighbor. If the distance is large, the data point is likely to be an anomaly.\n",
    "In this case, the data point has only 2 neighbors of the same class within a radius of 0.5. Since K=10, we need to find the distance between the data \n",
    "point and its 10th nearest neighbor. If the data point has only 2 neighbors within a radius of 0.5, it is unlikely that it will have 10 neighbors \n",
    "within the same radius. Therefore, we cannot compute the anomaly score of the data point using KNN with K=10.\n",
    "However, if we still want to compute the anomaly score using KNN with K=10, we can extend the distance radius until we find 10 neighbors. For example,\n",
    "if we extend the radius to 1, we may find 10 neighbors. We can then compute the distance between the data point and its 10th nearest neighbor and use\n",
    "it to compute the anomaly score. The larger the distance, the higher the anomaly score.\n",
    "Anomaly Score = 1 / (average distance to k nearest neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fe4e1-2f27-4bb0-927f-515ebf81bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "# anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "# length of the trees?\n",
    "Ans.\n",
    "The Isolation Forest algorithm generates a forest of decision trees, where each data point is isolated in a different partition of the feature space.\n",
    "The anomaly score of a data point is computed based on the average path length of the data point in the trees of the forest.\n",
    "If a data point has an average path length of 5.0 compared to the average path length of the trees, we can compute its anomaly score using the \n",
    "following formula:\n",
    "Anomaly Score = 2^(-average path length / c(n))\n",
    "\n",
    "where c(n) is a constant that depends on the number of data points n in the dataset. The value of c(n) can be computed as:\n",
    "c(n) = 2 * H(n-1) - (2 * (n-1) / n)\n",
    "where H(n-1) is the harmonic number of n-1.\n",
    "\n",
    "For a dataset of 3000 data points, c(n) can be computed as:\n",
    "c(3000) = 2 * H(2999) - (2 * 2999 / 3000) = 11.8979\n",
    "\n",
    "Using this value of c(n), we can compute the anomaly score of the data point with an average path length of 5.0 as:\n",
    "Anomaly Score = 2^(-5.0 / 11.8979) = 0.5017\n",
    "This indicates that the data point is less anomalous than a data point with an average path length that is farther from the average path length of the\n",
    "trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d701228-5bfc-4205-97ea-99c650573be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "IsolationForest(random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.randn(3000,10)\n",
    "\n",
    "clf = IsolationForest(n_estimators=100,contamination='auto',random_state=42)\n",
    "clf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd1f19a2-f63b-4948-909f-a446279cefd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.43270147, -0.44104147, -0.44888877, ..., -0.38783072,\n",
       "       -0.42319242, -0.45325706])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_scores = clf.score_samples(X)\n",
    "anomaly_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b029557-9b2f-4263-b2b0-f585fc8686af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The mean anomaly score is -0.4428\n"
     ]
    }
   ],
   "source": [
    "mean_anomaly_score = np.mean(anomaly_scores)\n",
    "# Print the mean anomaly score\n",
    "print(f\"\\nThe mean anomaly score is {mean_anomaly_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236aa3fb-a6f2-410b-b174-812d23fac2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e5a6c-667b-4cee-91b5-c4314d785555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
